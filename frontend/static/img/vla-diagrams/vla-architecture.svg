<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1000 700" width="1000" height="700">
  <style>
    .language { fill: #e3f2fd; stroke: #1976d2; stroke-width: 2; }
    .vision { fill: #e8f5e8; stroke: #388e3c; stroke-width: 2; }
    .action { fill: #fff3e0; stroke: #f57c00; stroke-width: 2; }
    .integration { fill: #f3e5f5; stroke: #c2185b; stroke-width: 2; }
    .ai { fill: #e1bee7; stroke: #7b1fa2; stroke-width: 2; }
    .text { font-family: Arial, sans-serif; font-size: 14px; font-weight: bold; fill: #333; }
    .label { font-family: Arial, sans-serif; font-size: 12px; fill: #666; }
    .arrow { stroke: #666; stroke-width: 2; marker-end: url(#arrowhead); }
  </style>

  <defs>
    <marker id="arrowhead" markerWidth="10" markerHeight="7"
            refX="10" refY="3.5" orient="auto">
      <polygon points="0 0, 10 3.5, 0 7" fill="#666" />
    </marker>
  </defs>

  <!-- Title -->
  <text x="500" y="40" class="text" text-anchor="middle" font-size="20">Vision-Language-Action (VLA) Architecture</text>

  <!-- Language Input -->
  <rect x="50" y="80" width="180" height="100" class="language" rx="10"/>
  <text x="140" y="110" class="text" text-anchor="middle">Language Input</text>
  <text x="140" y="130" class="label" text-anchor="middle">• Natural Language</text>
  <text x="140" y="145" class="label" text-anchor="middle">• Voice Commands</text>
  <text x="140" y="160" class="label" text-anchor="middle">• Text Prompts</text>

  <!-- Vision Input -->
  <rect x="50" y="220" width="180" height="100" class="vision" rx="10"/>
  <text x="140" y="250" class="text" text-anchor="middle">Vision Input</text>
  <text x="140" y="270" class="label" text-anchor="middle">• Camera Images</text>
  <text x="140" y="285" class="label" text-anchor="middle">• LiDAR Point Clouds</text>
  <text x="140" y="300" class="label" text-anchor="middle">• Sensor Data</text>

  <!-- LLM Processing -->
  <rect x="300" y="150" width="200" height="120" class="ai" rx="10"/>
  <text x="400" y="180" class="text" text-anchor="middle">Large Language Model</text>
  <text x="400" y="200" class="label" text-anchor="middle">• Understanding & Reasoning</text>
  <text x="400" y="215" class="label" text-anchor="middle">• Task Decomposition</text>
  <text x="400" y="230" class="label" text-anchor="middle">• Plan Generation</text>

  <!-- Vision Processing -->
  <rect x="300" y="300" width="200" height="100" class="vision" rx="10"/>
  <text x="400" y="330" class="text" text-anchor="middle">Vision Processing</text>
  <text x="400" y="350" class="label" text-anchor="middle">• Object Recognition</text>
  <text x="400" y="365" class="label" text-anchor="middle">• Scene Understanding</text>

  <!-- Action Planning -->
  <rect x="550" y="200" width="200" height="120" class="action" rx="10"/>
  <text x="650" y="230" class="text" text-anchor="middle">Action Planning</text>
  <text x="650" y="250" class="label" text-anchor="middle">• Task Sequencing</text>
  <text x="650" y="265" class="label" text-anchor="middle">• ROS 2 Mapping</text>
  <text x="650" y="280" class="label" text-anchor="middle">• Safety Validation</text>

  <!-- Execution -->
  <rect x="800" y="200" width="150" height="120" class="integration" rx="10"/>
  <text x="875" y="230" class="text" text-anchor="middle">ROS 2 Execution</text>
  <text x="875" y="250" class="label" text-anchor="middle">• Navigation</text>
  <text x="875" y="265" class="label" text-anchor="middle">• Manipulation</text>
  <text x="875" y="280" class="label" text-anchor="middle">• Control</text>

  <!-- Feedback Loop -->
  <rect x="550" y="350" width="200" height="80" class="integration" rx="10"/>
  <text x="650" y="380" class="text" text-anchor="middle">Perception Feedback</text>
  <text x="650" y="395" class="label" text-anchor="middle">• Execution Monitoring</text>
  <text x="650" y="410" class="label" text-anchor="middle">• State Updates</text>

  <!-- Arrows showing flow -->
  <line x1="230" y1="130" x2="300" y2="180" class="arrow"/>
  <line x1="230" y1="270" x2="300" y2="320" class="arrow"/>
  <line x1="500" y1="210" x2="550" y2="240" class="arrow"/>
  <line x1="500" y1="320" x2="550" y2="300" class="arrow"/>
  <line x1="750" y1="260" x2="800" y2="260" class="arrow"/>
  <line x1="875" y1="320" x2="875" y2="400" class="arrow"/>
  <line x1="800" y1="390" x2="750" y2="390" class="arrow"/>

  <!-- Process Labels -->
  <text x="100" y="60" class="label" font-weight="bold">Input Modalities</text>
  <text x="400" y="60" class="label" font-weight="bold">Processing & Understanding</text>
  <text x="650" y="60" class="label" font-weight="bold">Planning & Execution</text>
  <text x="875" y="60" class="label" font-weight="bold">Physical Action</text>

  <!-- Feedback Loop Label -->
  <text x="650" y="450" class="label" text-anchor="middle" font-weight="bold">Perception-Action Loop</text>

  <!-- Legend -->
  <rect x="50" y="600" width="900" height="80" fill="white" stroke="#ccc" rx="5"/>
  <text x="100" y="625" class="label" font-weight="bold">VLA Integration: Language understanding combined with visual perception enables AI agents to plan and execute physical actions through ROS 2</text>
  <text x="100" y="640" class="label">• Language models interpret natural commands and decompose tasks</text>
  <text x="100" y="655" class="label">• Vision systems provide environmental understanding and object grounding</text>
  <text x="100" y="670" class="label">• Action planning maps abstract goals to concrete ROS 2 commands</text>
</svg>